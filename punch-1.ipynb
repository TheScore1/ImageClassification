{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.7966177940368653, Validation Loss: 1.8067299127578735, Validation Accuracy: 0.16666666666666666\n",
      "Epoch 2, Train Loss: 1.7927348136901855, Validation Loss: 1.8049814105033875, Validation Accuracy: 0.16666666666666666\n",
      "Epoch 3, Train Loss: 1.7898041486740113, Validation Loss: 1.8014481663703918, Validation Accuracy: 0.2222222222222222\n",
      "Epoch 4, Train Loss: 1.7855790853500366, Validation Loss: 1.7997902035713196, Validation Accuracy: 0.25\n",
      "Epoch 5, Train Loss: 1.7841902017593383, Validation Loss: 1.7968769669532776, Validation Accuracy: 0.25\n",
      "Epoch 6, Train Loss: 1.7773647546768188, Validation Loss: 1.790298581123352, Validation Accuracy: 0.2777777777777778\n",
      "Epoch 7, Train Loss: 1.7634865760803222, Validation Loss: 1.7863752245903015, Validation Accuracy: 0.3611111111111111\n",
      "Epoch 8, Train Loss: 1.7603312253952026, Validation Loss: 1.7824226021766663, Validation Accuracy: 0.3888888888888889\n",
      "Epoch 9, Train Loss: 1.7761489868164062, Validation Loss: 1.781802237033844, Validation Accuracy: 0.3888888888888889\n",
      "Epoch 10, Train Loss: 1.7571723222732545, Validation Loss: 1.7780147790908813, Validation Accuracy: 0.3888888888888889\n",
      "Epoch 11, Train Loss: 1.7553241491317748, Validation Loss: 1.7685636281967163, Validation Accuracy: 0.3888888888888889\n",
      "Epoch 12, Train Loss: 1.7283949613571168, Validation Loss: 1.7640453577041626, Validation Accuracy: 0.3888888888888889\n",
      "Epoch 13, Train Loss: 1.7189407348632812, Validation Loss: 1.7606839537620544, Validation Accuracy: 0.4166666666666667\n",
      "Epoch 14, Train Loss: 1.737185573577881, Validation Loss: 1.76322340965271, Validation Accuracy: 0.3888888888888889\n",
      "Epoch 15, Train Loss: 1.6952991247177125, Validation Loss: 1.7361697554588318, Validation Accuracy: 0.4444444444444444\n",
      "Epoch 16, Train Loss: 1.6807145357131958, Validation Loss: 1.729258120059967, Validation Accuracy: 0.4444444444444444\n",
      "Epoch 17, Train Loss: 1.658386731147766, Validation Loss: 1.7226877212524414, Validation Accuracy: 0.4166666666666667\n",
      "Epoch 18, Train Loss: 1.689986252784729, Validation Loss: 1.6951091885566711, Validation Accuracy: 0.4444444444444444\n",
      "Epoch 19, Train Loss: 1.6471064567565918, Validation Loss: 1.6581199765205383, Validation Accuracy: 0.5\n",
      "Epoch 20, Train Loss: 1.6054929971694947, Validation Loss: 1.6566231846809387, Validation Accuracy: 0.5\n",
      "Epoch 21, Train Loss: 1.5764753103256226, Validation Loss: 1.646956443786621, Validation Accuracy: 0.5277777777777778\n",
      "Epoch 22, Train Loss: 1.532313346862793, Validation Loss: 1.6246384382247925, Validation Accuracy: 0.5277777777777778\n",
      "Epoch 23, Train Loss: 1.4707072019577025, Validation Loss: 1.5911235809326172, Validation Accuracy: 0.5277777777777778\n",
      "Epoch 24, Train Loss: 1.4457046747207642, Validation Loss: 1.5688989162445068, Validation Accuracy: 0.5277777777777778\n",
      "Epoch 25, Train Loss: 1.480912184715271, Validation Loss: 1.552743673324585, Validation Accuracy: 0.5277777777777778\n",
      "Epoch 26, Train Loss: 1.523965549468994, Validation Loss: 1.5231750011444092, Validation Accuracy: 0.5555555555555556\n",
      "Epoch 27, Train Loss: 1.4489770174026488, Validation Loss: 1.4802060723304749, Validation Accuracy: 0.6111111111111112\n",
      "Epoch 28, Train Loss: 1.3658669471740723, Validation Loss: 1.4557544589042664, Validation Accuracy: 0.6111111111111112\n",
      "Epoch 29, Train Loss: 1.3412969589233399, Validation Loss: 1.4348655343055725, Validation Accuracy: 0.6388888888888888\n",
      "Epoch 30, Train Loss: 1.4465109348297118, Validation Loss: 1.3870501518249512, Validation Accuracy: 0.6944444444444444\n",
      "Epoch 31, Train Loss: 1.289421844482422, Validation Loss: 1.3281015157699585, Validation Accuracy: 0.6666666666666666\n",
      "Epoch 32, Train Loss: 1.2118533849716187, Validation Loss: 1.3129353523254395, Validation Accuracy: 0.6666666666666666\n",
      "Epoch 33, Train Loss: 1.3420213460922241, Validation Loss: 1.2925199270248413, Validation Accuracy: 0.7222222222222222\n",
      "Epoch 34, Train Loss: 1.0948761463165284, Validation Loss: 1.2149589657783508, Validation Accuracy: 0.8055555555555556\n",
      "Epoch 35, Train Loss: 1.2548478484153747, Validation Loss: 1.1075437664985657, Validation Accuracy: 0.8055555555555556\n",
      "Epoch 36, Train Loss: 1.2400929093360902, Validation Loss: 1.0491695702075958, Validation Accuracy: 0.8055555555555556\n",
      "Epoch 37, Train Loss: 1.2879114627838135, Validation Loss: 1.0179266035556793, Validation Accuracy: 0.8333333333333334\n",
      "Epoch 38, Train Loss: 1.1160540223121642, Validation Loss: 0.8915726244449615, Validation Accuracy: 0.8055555555555556\n",
      "Epoch 39, Train Loss: 1.2160343170166015, Validation Loss: 0.784034252166748, Validation Accuracy: 0.8611111111111112\n",
      "Epoch 40, Train Loss: 1.045874559879303, Validation Loss: 0.7666121125221252, Validation Accuracy: 0.8611111111111112\n",
      "Epoch 41, Train Loss: 0.9367559075355529, Validation Loss: 0.7233058512210846, Validation Accuracy: 0.8611111111111112\n",
      "Epoch 42, Train Loss: 1.136569654941559, Validation Loss: 0.6288320124149323, Validation Accuracy: 0.8611111111111112\n",
      "Epoch 43, Train Loss: 0.797596025466919, Validation Loss: 0.5449988394975662, Validation Accuracy: 0.8333333333333334\n",
      "Epoch 44, Train Loss: 1.084514582157135, Validation Loss: 0.5209905058145523, Validation Accuracy: 0.8333333333333334\n",
      "Epoch 45, Train Loss: 0.8495303988456726, Validation Loss: 0.5283037424087524, Validation Accuracy: 0.8333333333333334\n",
      "Epoch 46, Train Loss: 0.9906044363975525, Validation Loss: 0.5279954969882965, Validation Accuracy: 0.8333333333333334\n",
      "Epoch 47, Train Loss: 0.6790924727916717, Validation Loss: 0.5353328585624695, Validation Accuracy: 0.8333333333333334\n",
      "Epoch 48, Train Loss: 0.8452479124069214, Validation Loss: 0.5151331424713135, Validation Accuracy: 0.8333333333333334\n",
      "Epoch 49, Train Loss: 0.9166757524013519, Validation Loss: 0.4746284782886505, Validation Accuracy: 0.8333333333333334\n",
      "Epoch 50, Train Loss: 0.6871419489383698, Validation Loss: 0.4391457587480545, Validation Accuracy: 0.8333333333333334\n",
      "Epoch 51, Train Loss: 1.2182762265205382, Validation Loss: 0.484524205327034, Validation Accuracy: 0.8333333333333334\n",
      "Epoch 52, Train Loss: 1.1057955026626587, Validation Loss: 0.5014659762382507, Validation Accuracy: 0.8333333333333334\n",
      "Epoch 53, Train Loss: 1.1033239722251893, Validation Loss: 0.49336737394332886, Validation Accuracy: 0.8333333333333334\n",
      "Epoch 54, Train Loss: 0.9758189439773559, Validation Loss: 0.5186896622180939, Validation Accuracy: 0.8333333333333334\n",
      "Epoch 55, Train Loss: 0.9334444880485535, Validation Loss: 0.49673034250736237, Validation Accuracy: 0.8333333333333334\n",
      "Epoch 56, Train Loss: 1.198814368247986, Validation Loss: 0.4964655637741089, Validation Accuracy: 0.8611111111111112\n",
      "Epoch 57, Train Loss: 0.8601132154464721, Validation Loss: 0.4872831404209137, Validation Accuracy: 0.8333333333333334\n",
      "Epoch 58, Train Loss: 0.8671575546264648, Validation Loss: 0.4683016538619995, Validation Accuracy: 0.8333333333333334\n",
      "Epoch 59, Train Loss: 0.9521094799041748, Validation Loss: 0.4382818043231964, Validation Accuracy: 0.8333333333333334\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "import numpy as np\n",
    "\n",
    "class OnePunchManDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.annotations['class'].unique())}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[idx, 1])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.class_to_idx[self.annotations.iloc[idx, 2]]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).cuda()\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def cutmix_data(x, y, alpha=1.0):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).cuda()\n",
    "    \n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n",
    "    \n",
    "    y_a, y_b = y, y[index]\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))\n",
    "    return x, y_a, y_b, lam\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomCrop(224, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "csv_file = 'one-punch-man/train.csv'\n",
    "root_dir = 'one-punch-man'\n",
    "dataset = OnePunchManDataset(csv_file=csv_file, root_dir=root_dir, transform=train_transform)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = val_transform\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class ImprovedCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ImprovedCNN, self).__init__()\n",
    "        self.efficientnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        num_features = self.efficientnet.classifier[1].in_features\n",
    "        self.efficientnet.classifier = nn.Identity()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.efficientnet(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "num_classes = len(dataset.class_to_idx)\n",
    "model = ImprovedCNN(num_classes=num_classes).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "scheduler = CyclicLR(optimizer, base_lr=0.00001, max_lr=0.0001, step_size_up=10, mode='triangular')\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, scheduler, alpha=1.0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        if np.random.rand() > 0.5:\n",
    "            images, labels_a, labels_b, lam = mixup_data(images, labels, alpha)\n",
    "            loss_fn = mixup_criterion\n",
    "        else:\n",
    "            images, labels_a, labels_b, lam = cutmix_data(images, labels, alpha)\n",
    "            loss_fn = mixup_criterion\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(criterion, outputs, labels_a, labels_b, lam)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return val_loss / len(val_loader), val_accuracy\n",
    "\n",
    "num_epochs = 60\n",
    "best_val_accuracy = 0.0\n",
    "patience = 10\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, scheduler, alpha=1.0)\n",
    "    val_loss, val_accuracy = validate(model, val_loader, criterion)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}')\n",
    "    \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "    \n",
    "    if counter >= patience:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "def predict(model, img_path, transform):\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).cuda()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    return predicted.item()\n",
    "\n",
    "test_folder = 'one-punch-man/test/test'\n",
    "predictions = []\n",
    "\n",
    "for img_name in os.listdir(test_folder):\n",
    "    if img_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        img_path = os.path.join(test_folder, img_name)\n",
    "        prediction = predict(model, img_path, val_transform)\n",
    "        predicted_class = list(dataset.class_to_idx.keys())[prediction]\n",
    "        predictions.append({'id': img_name.split('.')[0], 'path': img_name, 'class': predicted_class})\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.to_csv('predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
